{"cells":[{"cell_type":"markdown","metadata":{},"source":["### VARIABLE NAME AND THEIR DESCRIPTION"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:01:50.933984Z","iopub.status.busy":"2024-06-16T21:01:50.933512Z","iopub.status.idle":"2024-06-16T21:01:50.971056Z","shell.execute_reply":"2024-06-16T21:01:50.969852Z","shell.execute_reply.started":"2024-06-16T21:01:50.933952Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["do_file = r'/path/to/.DO/file'\n","variable_mapping = {}\n","\n","with open(do_file, 'r') as file:\n","    lines = file.readlines()\n","\n","# Parse label values\n","for line in lines:\n","    if line.startswith('label values'):\n","        parts = line.split()\n","        subvar = parts[2]\n","        var = parts[3]\n","        if var not in variable_mapping:\n","            variable_mapping[var] = []\n","        variable_mapping[var].append(subvar)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:01:50.976569Z","iopub.status.busy":"2024-06-16T21:01:50.976269Z","iopub.status.idle":"2024-06-16T21:01:51.433972Z","shell.execute_reply":"2024-06-16T21:01:51.432813Z","shell.execute_reply.started":"2024-06-16T21:01:50.976542Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["variable_info = {}\n","# Parse label variable\n","for line in lines:\n","    if line.startswith('label variable'):\n","        parts = line.split()\n","        subvar = parts[2]\n","        var_name = subvar.upper().split('_')[0]\n","        description = ' '.join(parts[3:]).strip('\"')\n","        if var_name in variable_mapping.keys():\n","            # Find the corresponding variable for the subvariable\n","            for var, subvars in variable_mapping.items():\n","                if subvar in subvars:\n","                    if var not in variable_info:\n","                        variable_info[var] = {'description': description, 'subnames': subvars, 'values': {}}\n","                    break\n","        else:\n","            if var_name not in variable_info:\n","                variable_info[var_name] = {'description': description, 'subnames': [subvar], 'values': {}}  "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:01:51.435457Z","iopub.status.busy":"2024-06-16T21:01:51.435159Z","iopub.status.idle":"2024-06-16T21:01:51.450483Z","shell.execute_reply":"2024-06-16T21:01:51.449346Z","shell.execute_reply.started":"2024-06-16T21:01:51.435430Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["current_var = None\n","for line in lines:\n","    if line.startswith('label define'):\n","        parts = line.split()\n","        current_var = parts[2]\n","    elif current_var and '\"' in line:\n","        value, desc = re.findall(r'(\\d+)\\s+\"([^\"]+)\"', line)[0]\n","        if current_var in variable_info:\n","            variable_info[current_var]['values'][int(value)] = desc"]},{"cell_type":"markdown","metadata":{},"source":["### VARIABLE COLUMN SPACINGS IN DAT file"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:01:51.452261Z","iopub.status.busy":"2024-06-16T21:01:51.451818Z","iopub.status.idle":"2024-06-16T21:01:51.493863Z","shell.execute_reply":"2024-06-16T21:01:51.492686Z","shell.execute_reply.started":"2024-06-16T21:01:51.452232Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["dct_file = r'path/to/.DCT/file'\n","column_info = []\n","\n","with open(dct_file, 'r') as file:\n","    lines = file.readlines()\n","\n","for line in lines[2:-1]:\n","    parts = line.split()\n","    if len(parts) == 4:\n","        var_type = parts[0]\n","        subname = parts[1]\n","        start_pos = int(parts[3].split('-')[0]) - 1\n","        end_pos = int(parts[3].split('-')[1])\n","        column_info.append((subname, var_type, start_pos, end_pos))\n","    elif len(parts) == 3:\n","        var_type = parts[0]\n","        subname = parts[1]\n","        start_pos = int(parts[2].split(':')[1].split('-')[0]) - 1\n","        end_pos = int(parts[2].split(':')[1].split('-')[1])\n","        column_info.append((subname, var_type, start_pos, end_pos))\n","    else:\n","        print(\"ERROR: Invalid line:\", line)"]},{"cell_type":"markdown","metadata":{},"source":["### READ GEO FILE TO GET COLUMNS TO READ"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:01:51.495621Z","iopub.status.busy":"2024-06-16T21:01:51.495260Z","iopub.status.idle":"2024-06-16T21:02:05.113454Z","shell.execute_reply":"2024-06-16T21:02:05.112169Z","shell.execute_reply.started":"2024-06-16T21:01:51.495588Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["import geopandas as gpd\n","\n","# Path to the shapefile\n","shapefile_path = r\"path/to/.SHP/file\"\n","\n","# Load the shapefile\n","gdf = gpd.read_file(shapefile_path)\n","\n","\n","# Extract only the necessary columns from the GPS data (cluster number and coordinates)\n","gdf = gdf[['DHSCLUST', 'LATNUM', 'LONGNUM']]\n","\n","# Define new bounds for bombay\n","top_left = {'lat': 19.269802785051485, 'long': 72.7749741438024}\n","bottom_right = {'lat': 18.894288353461903, 'long': 72.9847334143265}\n","\n","# Filter the geodataframe for clusters within these bounds\n","gdf = gdf[\n","    (gdf['LATNUM'] <= top_left['lat']) & \n","    (gdf['LATNUM'] >= bottom_right['lat']) & \n","    (gdf['LONGNUM'] >= top_left['long']) & \n","    (gdf['LONGNUM'] <= bottom_right['long'])\n","]\n","\n","# Get the DHSCLUST values within the bounds\n","dhsclust_within_bounds = gdf['DHSCLUST'].tolist()\n","print(\"DHSCLUST values within bounds:\", dhsclust_within_bounds)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:02:05.115266Z","iopub.status.busy":"2024-06-16T21:02:05.114855Z","iopub.status.idle":"2024-06-16T21:02:05.132019Z","shell.execute_reply":"2024-06-16T21:02:05.130867Z","shell.execute_reply.started":"2024-06-16T21:02:05.115229Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["gdf.head()"]},{"cell_type":"markdown","metadata":{},"source":["### READ DAT FILE FOR VALUES"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:02:05.134202Z","iopub.status.busy":"2024-06-16T21:02:05.133768Z","iopub.status.idle":"2024-06-16T21:04:19.237137Z","shell.execute_reply":"2024-06-16T21:04:19.236135Z","shell.execute_reply.started":"2024-06-16T21:02:05.134163Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["data_file = 'path/to/.DAT/file'\n","data = []\n","\n","with open(data_file, 'r') as file:\n","    for line in file:\n","        record = {}\n","        cluster_value = int(line[column_info[2][2]:column_info[2][3]].strip())  # v001 is the third variable\n","        if cluster_value in dhsclust_within_bounds:\n","            for subname, var_type, start_pos, end_pos in column_info:\n","                value = line[start_pos:end_pos].strip()\n","                try:\n","                    if var_type in ['int', 'byte', 'long']:\n","                        value = int(value)\n","                    elif var_type == 'float':\n","                        value = float(value)\n","                except ValueError:\n","                    value = pd.NA\n","                record[subname] = value\n","            data.append(record)"]},{"cell_type":"markdown","metadata":{},"source":["### CREATE DATAFRAME"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:04:19.240756Z","iopub.status.busy":"2024-06-16T21:04:19.240411Z","iopub.status.idle":"2024-06-16T21:04:25.609743Z","shell.execute_reply":"2024-06-16T21:04:25.608440Z","shell.execute_reply.started":"2024-06-16T21:04:19.240720Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# Create DataFrame from data\n","df = pd.DataFrame(data)\n","\n","averaged_data = {}\n","\n","for var_name, info in variable_info.items():\n","    subnames = info['subnames']\n","    if subnames:\n","        # If only one subname exists\n","        if len(subnames) == 1:\n","            subname = subnames[0]\n","            # Check if the data type of the subname is numeric\n","            if pd.api.types.is_numeric_dtype(df[subname]):\n","                averaged_data[var_name] = df[subname]\n","        else:\n","            # Check if any of the subname columns are numeric\n","            if not df[subnames].select_dtypes(include=['number']).empty:\n","                # Average the subname values, skipping NaNs\n","                averaged_data[var_name] = df[subnames].mean(axis=1, skipna=True)\n","\n","# Create the final DataFrame\n","averaged_df = pd.DataFrame(averaged_data)\n","\n","# Print the DataFrame\n","print(averaged_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:04:25.611494Z","iopub.status.busy":"2024-06-16T21:04:25.611094Z","iopub.status.idle":"2024-06-16T21:04:25.639111Z","shell.execute_reply":"2024-06-16T21:04:25.637856Z","shell.execute_reply.started":"2024-06-16T21:04:25.611458Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# Merge the DataFrames\n","merged_df = pd.merge(gdf, averaged_df, how='inner', left_on='DHSCLUST', right_on='HV001')\n","\n","# Print the merged DataFrame\n","print(merged_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:04:25.640830Z","iopub.status.busy":"2024-06-16T21:04:25.640500Z","iopub.status.idle":"2024-06-16T21:04:25.917904Z","shell.execute_reply":"2024-06-16T21:04:25.916724Z","shell.execute_reply.started":"2024-06-16T21:04:25.640800Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# Group by DHSCLUST and calculate the mean for each numeric column\n","merged_df = merged_df.groupby('DHSCLUST').mean()\n","merged_df"]},{"cell_type":"markdown","metadata":{},"source":["### LINK THE DATAFRAME AND HEIGHT VALUES"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:04:25.919543Z","iopub.status.busy":"2024-06-16T21:04:25.919217Z","iopub.status.idle":"2024-06-16T21:04:25.923981Z","shell.execute_reply":"2024-06-16T21:04:25.922867Z","shell.execute_reply.started":"2024-06-16T21:04:25.919514Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["pixel_radius = 2000//5"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:04:25.926172Z","iopub.status.busy":"2024-06-16T21:04:25.925729Z","iopub.status.idle":"2024-06-16T21:04:28.645176Z","shell.execute_reply":"2024-06-16T21:04:28.641575Z","shell.execute_reply.started":"2024-06-16T21:04:25.926139Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["import rasterio\n","import geopandas as gpd\n","from pyproj import Transformer\n","import numpy as np\n","from sklearn.metrics import r2_score\n","import matplotlib.pyplot as plt\n","\n","# Function to compute average intensity within a radius\n","def average_intensity(image_data, row, col, pixel_radius):\n","    # Calculate the bounds of the window around the specified center (row, col)\n","    start_row = max(row - pixel_radius, 0)\n","    end_row = min(row + pixel_radius + 1, image_data.shape[0])\n","    start_col = max(col - pixel_radius, 0)\n","    end_col = min(col + pixel_radius + 1, image_data.shape[1])\n","\n","    # Extract the subarray from the image data\n","    subarray = image_data[start_row:end_row, start_col:end_col]\n","    \n","    # Compute the mean\n","    return np.mean(subarray)\n","\n","# Path to the raster file (make sure to use the correct path)\n","raster_path = '/path/to/target/image'\n","\n","# Load the raster file using rasterio\n","with rasterio.open(raster_path) as src:\n","    affine = src.transform  # Get the affine transformation of the raster\n","    image_data = src.read(1)  # Read the first band of the raster\n","\n","# Confirm that the data is loaded by checking the shape\n","print(\"Raster dimensions:\", image_data.shape)\n","\n","# Setup Transformer to convert from WGS84 to UTM43N\n","transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32643\", always_xy=True)\n","\n","# Assuming you have a GeoDataFrame `gdf` with columns 'LATNUM' and 'LONGNUM' for coordinates\n","# Prepare to add building heights to the GeoDataFrame\n","heights = []\n","\n","for index, row in merged_df.iterrows():\n","    # Convert geographic (longitude, latitude) to UTM coordinates\n","    utm_x, utm_y = transformer.transform(row['LONGNUM'], row['LATNUM'])\n","    \n","    # Convert UTM coordinates to raster indices\n","    col, row_idx = ~affine * (utm_x, utm_y)\n","    col, row_idx = int(col), int(row_idx)\n","    \n","    # Check if the raster indices are within the bounds of the raster image\n","    if 0 <= row_idx < image_data.shape[0] and 0 <= col < image_data.shape[1]:\n","        # Append building height at the raster index to the list\n","        heights.append(average_intensity(image_data, row_idx, col, pixel_radius))\n","    else:\n","        # Append NaN if indices are out of bounds\n","        heights.append(np.nan)\n","        # Print relevant parameters for debugging\n","        print(f\"Out of bounds: LATNUM={row['LATNUM']}, LONGNUM={row['LONGNUM']}, UTM_X={utm_x}, UTM_Y={utm_y}, Col={col}, Row={row_idx}\")\n","\n","# Assign the list of building heights to the GeoDataFrame\n","merged_df['building_height'] = heights\n","# Drop rows where building height is NaN\n","merged_df = merged_df.dropna(subset=['building_height'])\n","# Display the updated GeoDataFrame\n","print(merged_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["### PLOT SCATTER PLOTS FOR VARIABLES"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the variables to plot against building_height\n","variables = ['define', 'your', 'variables of interest']\n","colors = ['blue', 'green','orange', 'purple']  # Colors for each plot\n","\n","# Loop through variables for plotting and correlation\n","for i, var in enumerate(variables):\n","    correlation = merged_df['building_height'].corr(merged_df[var])\n","    plt.figure(figsize=(6, 4))\n","    plt.scatter(merged_df['building_height'], merged_df[var], color=colors[i])\n","    plt.title(f'Building Height vs {var} - Correlation: {correlation:.2f}')\n","    plt.xlabel('Building Height')\n","    plt.ylabel(var)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### PLOT BAR CHARTS BETWEEN CLUSTER VALUE AND CLUSTER NUMBERS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import rasterio\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pyproj import Transformer\n","from tqdm import tqdm\n","\n","# Step 1: Load the clustered raster data\n","clustered_raster_path = '/path/to/clustered/image'\n","with rasterio.open(clustered_raster_path) as src:\n","    clustered_image = src.read(1)  # Read the first band\n","    transform = src.transform\n","\n","# Step 2: Convert UTM43N coordinates to pixel coordinates\n","def utm43n_to_pixel(lon, lat, transform):\n","    transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:32643\", always_xy=True)\n","    utm_x, utm_y = transformer.transform(lon, lat)\n","    col, row = ~transform * (utm_x, utm_y)\n","    return int(row), int(col)\n","\n","# Step 3: Assign each household to a cluster\n","clusters = []\n","for idx, row in tqdm(merged_df.iterrows(), total=merged_df.shape[0]):\n","    lat, lon, height = row['LATNUM'], row['LONGNUM'], row['building_height']\n","    pixel_row, pixel_col = utm43n_to_pixel(lon, lat, transform)\n","    try:\n","        cluster_value = clustered_image[pixel_row, pixel_col]\n","        clusters.append(cluster_value)\n","    except IndexError:\n","        clusters.append(-1)  # If the household is out of the raster bounds\n","\n","merged_df.loc[:, 'cluster'] = clusters\n","\n","# Filter out households that are out of bounds and exclude cluster -1 and 255\n","filtered_df = merged_df[(merged_df['cluster'] != -1) & (merged_df['cluster'] != 255)]\n","\n","# Define the variables list\n","variables = ['define', 'your', 'variables of interest']\n","\n","# Step 4: Plot each variable against clusters\n","for variable in variables:\n","    # Group by cluster and get the value counts for each variable\n","    grouped_counts = filtered_df.groupby('cluster')[variable].value_counts().unstack(fill_value=0)\n","\n","    # Calculate average values for each cluster\n","    average_values = filtered_df.groupby('cluster')[variable].mean()\n","\n","    # Create a bar plot\n","    ax = grouped_counts.plot(kind='bar', stacked=True, figsize=(12, 8))\n","\n","    ax.set_title(f'Distribution of {variable} by Cluster')\n","    ax.set_xlabel('Cluster')\n","    ax.set_ylabel('Count')\n","    \n","    # Add average values to the legend\n","    avg_labels = [f'Avg for Cluster {i}: {avg:.2f}' for i, avg in average_values.items()]\n","    ax.legend(title=variable, labels=avg_labels, loc='upper right')\n","\n","    plt.show()\n","    plt.close()"]},{"cell_type":"markdown","metadata":{},"source":["### FILTER THE RELEVANT VARIABLES USING A LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:04:28.652349Z","iopub.status.busy":"2024-06-16T21:04:28.650857Z","iopub.status.idle":"2024-06-16T21:04:28.730381Z","shell.execute_reply":"2024-06-16T21:04:28.729119Z","shell.execute_reply.started":"2024-06-16T21:04:28.652302Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["import requests\n","import time\n","\n","# Hugging Face model endpoint\n","model_name = \"gpt2\"\n","\n","# Hugging Face API URL\n","url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n","\n","# Obtain your Hugging Face Bearer token from environment variables or elsewhere\n","token = 'your token'\n","\n","# List of non-relevant terms\n","non_relevant_terms = [\n","    \"cluster number\", \"district\", \"aadhaar\", \"phone number\", \"ID\", \"identification\", \n","    \"line number\", \"address\", \"record number\", \"identifier\", \"supervisor\",\n","    \"language\", \"error\", \"area unit\", \"day of\", \"date of\", \"month of\", \"year of\"\n","]\n","\n","def is_relevant(description):\n","    # Check if the description contains any non-relevant term\n","    if any(term in description.lower() for term in non_relevant_terms):\n","        return False\n","\n","    headers = {\n","        \"Authorization\": f\"Bearer {token}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    payload = {\n","        \"inputs\": (\n","            f\"Consider a variable description from the DHS VII survey dataset for India. \"\n","            f\"Determine if the variable is related to socio-economic or health indicators. \"\n","            f\"Examples of irrelevant variables include: cluster number, aadhaar number, phone number, state, district, district number and supervisor ID. \"\n","            f\"Examples of relevant variables include: education level, number of household members, wall/floor material, source of water, health measures like BMI, wealth measures, drainage facilities, and access to hospitals. \"\n","            f\"Now, given the following variable description, is it relevant to socio-economic or health indicators? Description: {description}. Respond with 'Yes' or 'No'.\"\n","        ),\n","        \"parameters\": {\n","            \"max_new_tokens\": 100,\n","            \"do_sample\": False,\n","            \"return_full_text\": False,\n","            \"stop_sequence\": \"'\"\n","        }\n","    }\n","\n","    retries = 10  # Number of retries\n","    backoff_time = 10  # Initial backoff time in seconds\n","\n","    for attempt in range(retries):\n","        try:\n","            response = requests.post(url, headers=headers, json=payload)\n","            response.raise_for_status()  # Raise HTTPError for bad responses\n","\n","            # Assuming response_json is a list and we want the first element\n","            response_json = response.json()\n","            if isinstance(response_json, list):\n","                generated_text = response_json[0]['generated_text']\n","            else:\n","                generated_text = response_json['generated_text']\n","\n","            return 'Yes' in generated_text\n","\n","        except requests.HTTPError as e:\n","            if response.status_code == 429:\n","                # Handle rate limiting\n","                print(f\"Rate limited. Retrying in {backoff_time} seconds.\")\n","                time.sleep(backoff_time)\n","                backoff_time *= 2  # Exponential backoff\n","            else:\n","                # For other HTTP errors, try again after a short delay\n","                print(f\"HTTP Error occurred: {e}. Retrying...\")\n","                time.sleep(2)\n","        except (KeyError, IndexError, requests.RequestException) as e:\n","            # For other errors, try again after a short delay\n","            print(f\"Error occurred: {e}. Retrying...\")\n","            time.sleep(2)\n","\n","    # If the maximum retries are reached, log and return False\n","    print(\"Reached maximum retries. Please try again later.\")\n","    return False"]},{"cell_type":"markdown","metadata":{},"source":["### CREATE CORRELATION TABLE FOR RELEVANT VARIABLES"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:04:28.739898Z","iopub.status.busy":"2024-06-16T21:04:28.739493Z","iopub.status.idle":"2024-06-16T21:11:01.176121Z","shell.execute_reply":"2024-06-16T21:11:01.175009Z","shell.execute_reply.started":"2024-06-16T21:04:28.739858Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# Define a list to store correlations\n","correlation_list = []\n","\n","# Iterate over each variable in variable_info\n","for var_name, info in variable_info.items():\n","    # Check if the variable exists as a column in merged_df\n","    if var_name in merged_df.columns:\n","        if is_relevant(info['description']) :\n","            # Calculate the correlation between the variable and building_height\n","            correlation = merged_df[var_name].corr(merged_df['building_height'])\n","            # Store the correlation value along with the variable name and description\n","            correlation_list.append((var_name, info['description'], info['values'], correlation))\n","        else:\n","            print(\"NOT RELEVANT\", var_name, info['description'])\n","\n","# Create a DataFrame from the correlation list\n","correlation_df = pd.DataFrame(correlation_list, columns=['Variable Name', 'Description', 'Range of Values', 'Correlation'])\n","\n","# Sort the correlations in descending order based on their absolute values\n","correlation_df['Absolute Correlation'] = correlation_df['Correlation'].abs()\n","correlation_df = correlation_df.sort_values(by='Absolute Correlation', ascending=False).drop(columns='Absolute Correlation')"]},{"cell_type":"markdown","metadata":{},"source":["### TABULATE THE FINAL CORRELATION TABLE"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-06-16T21:11:01.178210Z","iopub.status.busy":"2024-06-16T21:11:01.177872Z","iopub.status.idle":"2024-06-16T21:11:01.306195Z","shell.execute_reply":"2024-06-16T21:11:01.304907Z","shell.execute_reply.started":"2024-06-16T21:11:01.178180Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from tabulate import tabulate\n","\n","\n","# Convert correlation DataFrame to tabular format\n","correlation_table = correlation_df[['Variable Name', 'Description', 'Range of Values', 'Correlation']].values.tolist()\n","\n","# Format the table using tabulate\n","formatted_table = tabulate(correlation_table, headers=['Variable Name', 'Description', 'Range of Values', 'Correlation'], tablefmt='grid')\n","\n","# Print the table (optional)\n","print(formatted_table)\n","\n","# Save the table to a text file\n","with open('MUMBAI_IR_RANKED_CORRELARIONS.txt', 'w') as f:\n","    f.write(formatted_table)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4465767,"sourceId":7659180,"sourceType":"datasetVersion"},{"datasetId":5101554,"sourceId":8539797,"sourceType":"datasetVersion"},{"datasetId":5102125,"sourceId":8540523,"sourceType":"datasetVersion"},{"datasetId":5103219,"sourceId":8542001,"sourceType":"datasetVersion"},{"datasetId":5103230,"sourceId":8542016,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
